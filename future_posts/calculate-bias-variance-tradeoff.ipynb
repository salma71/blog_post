{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":11,"outputs":[{"output_type":"stream","text":"/kaggle/input/logistic-reg-visual/Social_Network_Ads.csv\n/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe performance of a machine learning model is based on two factors, bias and variance. The ultimate goal for any predictive modeling is to find the perfect spot where the model has low bias and low variance. However, in practice it is a challenging task. We can simple reduce bias by increasing variance and this may lead to an overfitting issue or decreasing variance to increase bias which may results in underfitting. Both cases may cause a defective predictive model. There is an equilibrium state where the increasing/decreasing in bias is equivalent to the reduction in bias and the same for other way around; this state called the Bias-Variance Trade-off.\n\nThere is no direct statistical way to calculate the Bias-Variance Trade-off for a predictive model. However, understanding the concept well would help build a robust predictive model that is close to the actual one. \n\n"},{"metadata":{},"cell_type":"markdown","source":"# Understanding the concepts\nI would start by comprehend the intuition for each term in the machine learning predictive model. \n\n## What does Bias means?\nA high bias assumption assums that the output is strongle depends on the inputs. It is a measure of how close that assumption to acheive the true relationship between predictions and the outcomes. Here are two examples of both high and low bias.\n\n**High-Bias**: Linear regression models are the best example that represent a high bias characteristics; however, it has a weak assumptions .\n\n**Low-Bias** strongly represented in the Non-Linear regression models; however, it forms a strong assumpution between inputs and the outcome.\n\n## What does Variance means?\nIt is a reference of how the model would perform when fit on different training set. It catches the impact of specifics that the data has on the model. \n\n**High-Variance**: has a significant impact on changing the training set.\n\n**Low-Bias** has a slight impact on changing the training set.\n"},{"metadata":{},"cell_type":"markdown","source":"## What is model error means?\n\nIn machine learning, each model has an error; this error is consists of two errors. \n\n### Model error\n1. Reducible error\n\n    1.1 Variance error\n    \n    1.2 Bias error\n    \n    \n2. Irreducible error\n\nLet's break them down one by one. \n### Reducible error charachertistics\n\n* Defined by the quantity we decrease when learning on the training set\n* We can improve\n* Should be close to zero as possible as we could\n\n### Irreducible error characteristics\n\n* Can't be eliminated\n* Caused by uncontoled factors - aka statistical noise\n* Represent the upper bound on the accuracy when predicting y_hat"},{"metadata":{},"cell_type":"markdown","source":"## How do we keep track of the trade-off value in the model?\n\nAs I mentioned earlier, the ultimate goal for any predictive model is to reach the equilibrium state trade-off value for both bias and variance. \n* **Linear relationships** have high bias, low variance -> may results in underfitting issue -> aka linear regression\n* **Complex relationships** have low bias, high variance -> may causes over fitting -> aka: randomforest\n\nThere are some parameters we can control to tune the bias-variance trade-off. For example, `k` in the nearest neighbor algorithm. A lower value of `k` would results in a low bias/high variance. However, as we increase the `k` the model would become significantly biased with low variance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating the bias-variance trade-off\n\nIn the following demo, I will demonestrate the by example, how to calculate the ration using the `mlxtend` library via `bias_variance_decomp()` function within the `evaluate` class. The library was developed by [Sebastian Raschka](https://sebastianraschka.com/) and it is easy to use. The fuction can estimate the bias and variance for a model over multiple bootstrap samples. I will use the `diabetes` dataset within `sklearn` library. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# estimate the bias and variance for a regression model\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom mlxtend.evaluate import bias_variance_decomp\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\n# load the data from sklearn\nX, y = load_diabetes(return_X_y=True)\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# define the model\nmodel = LinearRegression()\n# estimate bias and variance\nmse, bias, var = bias_variance_decomp(model, X_train, y_train, X_test, y_test, loss='mse', num_rounds=200, random_seed=1)\n# summarize results\nprint('MSE: %.3f' % mse)\nprint('Bias: %.3f' % bias)\nprint('Variance: %.3f' % var)\n","execution_count":17,"outputs":[{"output_type":"stream","text":"MSE: 2918.740\nBias: 2810.376\nVariance: 108.364\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As shown in the results, as expected, the model has a high bias, low variance - simple model. Also, the model error `MSE` is the summation of bias and variance as we stated earlier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# variance + bias = mse \n2810.376 + 108.364","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"2918.7400000000002"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion and take aways\n\n* When developing a machine learning model, what we care about is the overall error. The perfect spot is the level of model complexity at which the increase in bias is equivelant to the reduction in variance to satisfy model regidity condition. \n\n* Can we calculate the exact bias/variance? We can't because we don't have the true functional mapping for a predictive modeling problem.\n\n* What we can is to use the irreducible error, bias-variance trade-off to configure which model is the best and interpret the results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}